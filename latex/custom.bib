% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{lin2022pretrained,
  title={Pretrained transformers for text ranking: Bert and beyond},
  author={Lin, Jimmy and Nogueira, Rodrigo and Yates, Andrew},
  year={2022},
  publisher={Springer Nature}
}

@inproceedings{robertson1994okapi,
  author       = {Stephen E. Robertson and
                  Steve Walker and
                  Susan Jones and
                  Micheline Hancock{-}Beaulieu and
                  Mike Gatford},
  editor       = {Donna K. Harman},
  title        = {Okapi at {TREC-3}},
  booktitle    = {Proceedings of The Third Text REtrieval Conference, {TREC} 1994, Gaithersburg,
                  Maryland, USA, November 2-4, 1994},
  series       = {{NIST} Special Publication},
  volume       = {500-225},
  pages        = {109--126},
  publisher    = {National Institute of Standards and Technology {(NIST)}},
  year         = {1994},
  url          = {http://trec.nist.gov/pubs/trec3/papers/city.ps.gz},
  timestamp    = {Wed, 07 Jul 2021 16:44:22 +0200},
  biburl       = {https://dblp.org/rec/conf/trec/RobertsonWJHG94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{shi2024replug,
    title = "{REPLUG}: Retrieval-Augmented Black-Box Language Models",
    author = "Shi, Weijia  and
      Min, Sewon  and
      Yasunaga, Michihiro  and
      Seo, Minjoon  and
      James, Richard  and
      Lewis, Mike  and
      Zettlemoyer, Luke  and
      Yih, Wen-tau",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.463/",
    doi = "10.18653/v1/2024.naacl-long.463",
    pages = "8371--8384"
}



@article{ram2023incontext,
    author = {Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
    title = {In-Context Retrieval-Augmented Language Models},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {11},
    pages = {1316-1331},
    year = {2023},
    month = {11},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00605},
    url = {https://doi.org/10.1162/tacl\_a\_00605},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00605/2178834/tacl\_a\_00605.pdf},
}



@article{izacard2023atlas,
  author  = {Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
  title   = {Atlas: Few-shot Learning with Retrieval Augmented Language Models},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {251},
  pages   = {1--43},
  url     = {http://jmlr.org/papers/v24/23-0037.html}
}


@inproceedings{lewis2020retrieval,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
 volume = {33},
 year = {2020}
}



@inproceedings{taule2008ancora,
    title = "{A}n{C}ora: Multilevel Annotated Corpora for {C}atalan and {S}panish",
    author = "Taul{\'e}, Mariona  and
      Mart{\'\i}, M. Ant{\`o}nia  and
      Recasens, Marta",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/35_paper.pdf",
}


@inproceedings{rasley2020deepspeed,
  author    = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  title     = {DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
  year      = {2020},
  isbn      = {9781450379984},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3394486.3406703},
  doi       = {10.1145/3394486.3406703},
  booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages     = {3505–3506},
  numpages  = {2},
  keywords  = {distributed deep learning, machine learning},
  location  = {Virtual Event, CA, USA},
  series    = {KDD '20}
}
@inproceedings{wolf2020transformers,
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  author    = {Wolf, Thomas  and
               Debut, Lysandre  and
               Sanh, Victor  and
               Chaumond, Julien  and
               Delangue, Clement  and
               Moi, Anthony  and
               Cistac, Pierric  and
               Rault, Tim  and
               Louf, Remi  and
               Funtowicz, Morgan  and
               Davison, Joe  and
               Shleifer, Sam  and
               von Platen, Patrick  and
               Ma, Clara  and
               Jernite, Yacine  and
               Plu, Julien  and
               Xu, Canwen  and
               Le Scao, Teven  and
               Gugger, Sylvain  and
               Drame, Mariama  and
               Lhoest, Quentin  and
               Rush, Alexander},
  editor    = {Liu, Qun  and
               Schlangen, David},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = oct,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-demos.6},
  doi       = {10.18653/v1/2020.emnlp-demos.6},
  pages     = {38--45}
}

@Book{jurafsky2024slp,
  author  = {Daniel Jurafsky and James H. Martin},
  title   = {Speech and Language Processing: An Introduction to
             Natural Language Processing, Computational Linguistics,
             and Speech Recognition with Language Models},
  year    = {2024},
  url     = {https://web.stanford.edu/~jurafsky/slp3/},
  note    = {Online manuscript released August 20, 2024},
  edition = {3rd}
}

@article{grootendorst2022bertopic,
  title   = {BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
  author  = {Grootendorst, Maarten},
  journal = {arXiv preprint arXiv:2203.05794},
  year    = {2022},
  url     = {https://arxiv.org/abs/2203.05794}
}

@article{wang2024text,
      title={Text Embeddings by Weakly-Supervised Contrastive Pre-training}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
      year={2024},
      journal = {arXiv preprint arXiv:2212.03533},
      url={https://arxiv.org/abs/2212.03533}, 
}


@inproceedings{zhang2021mr,
  title     = {Mr. {T}y{D}i: A Multi-lingual Benchmark for Dense Retrieval},
  author    = {Zhang, Xinyu  and
               Ma, Xueguang  and
               Shi, Peng  and
               Lin, Jimmy},
  editor    = {Ataman, Duygu  and
               Birch, Alexandra  and
               Conneau, Alexis  and
               Firat, Orhan  and
               Ruder, Sebastian  and
               Sahin, Gozde Gul},
  booktitle = {Proceedings of the 1st Workshop on Multilingual Representation Learning},
  month     = nov,
  year      = {2021},
  address   = {Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.mrl-1.12},
  doi       = {10.18653/v1/2021.mrl-1.12},
  pages     = {127--137}
}

@book{fleiss2013statistical,
  title     = {Statistical methods for rates and proportions},
  author    = {Fleiss, Joseph L and Levin, Bruce and Paik, Myunghee Cho},
  year      = {2013},
  publisher = {john wiley \& sons}
}

@inproceedings{Lin_etal_SIGIR2021_Pyserini,
  author    = {Jimmy Lin and Xueguang Ma and Sheng-Chieh Lin and Jheng-Hong Yang and Ronak Pradeep and Rodrigo Nogueira},
  title     = {{Pyserini}: A {Python} Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations},
  booktitle = {Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021)},
  year      = 2021,
  pages     = {2356--2362},
  url       = {https://dl.acm.org/doi/10.1145/3404835.3463238}
}

@article{robertson2009probabilistic,
  title     = {The probabilistic relevance framework: BM25 and beyond},
  author    = {Robertson, Stephen and Zaragoza, Hugo and others},
  journal   = {Foundations and Trends{\textregistered} in Information Retrieval},
  volume    = {3},
  number    = {4},
  pages     = {333--389},
  year      = {2009},
  publisher = {Now Publishers, Inc.},
  url       = {https://dl.acm.org/doi/10.1561/1500000019}
}

@inproceedings{thakur2021beir,
  title     = {{BEIR}: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},
  author    = {Nandan Thakur and Nils Reimers and Andreas R{\"u}ckl{\'e} and Abhishek Srivastava and Iryna Gurevych},
  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year      = {2021},
  url       = {https://openreview.net/forum?id=wCu6T5xFjeJ}
}

@article{wang2024multilingual,
  title   = {Multilingual e5 text embeddings: A technical report},
  author  = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal = {arXiv preprint arXiv:2402.05672},
  year    = {2024},
  url     = {https://arxiv.org/abs/2402.05672}
}

@article{neelakantan2022text,
  title   = {Text and code embeddings by contrastive pre-training},
  author  = {Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris and others},
  journal = {arXiv preprint arXiv:2201.10005},
  year    = {2022},
  url     = {https://cdn.openai.com/papers/Text_and_Code_Embeddings_by_Contrastive_Pre_Training.pdf}
}

@inproceedings{muennighoff-etal-2023-mteb,
  title     = {{MTEB}: Massive Text Embedding Benchmark},
  author    = {Muennighoff, Niklas  and
               Tazi, Nouamane  and
               Magne, Loic  and
               Reimers, Nils},
  editor    = {Vlachos, Andreas  and
               Augenstein, Isabelle},
  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  month     = may,
  year      = {2023},
  address   = {Dubrovnik, Croatia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.eacl-main.148},
  doi       = {10.18653/v1/2023.eacl-main.148},
  pages     = {2014--2037},
  abstract  = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings todate. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-theart results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at \url{https://github.com/embeddings-benchmark/mteb}.}
}

@book{eberhard2024ethnologue,
  author    = {David M. Eberhard and Gary F. Simons and Charles D. Fennig},
  address   = {Dallas},
  edition   = {27},
  publisher = {SIL International},
  title     = {Ethnologue: Languages of the World},
  url       = {http://www.ethnologue.com},
  year      = {2024}
}

@inproceedings{jiang2023active,
  title     = {Active Retrieval Augmented Generation},
  author    = {Jiang, Zhengbao  and
               Xu, Frank  and
               Gao, Luyu  and
               Sun, Zhiqing  and
               Liu, Qian  and
               Dwivedi-Yu, Jane  and
               Yang, Yiming  and
               Callan, Jamie  and
               Neubig, Graham},
  editor    = {Bouamor, Houda  and
               Pino, Juan  and
               Bali, Kalika},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.emnlp-main.495},
  doi       = {10.18653/v1/2023.emnlp-main.495},
  pages     = {7969--7992}
}

@inproceedings{cheng2023uprise,
  title     = {{UPRISE}: Universal Prompt Retrieval for Improving Zero-Shot Evaluation},
  author    = {Cheng, Daixuan  and
               Huang, Shaohan  and
               Bi, Junyu  and
               Zhan, Yuefeng  and
               Liu, Jianfeng  and
               Wang, Yujing  and
               Sun, Hao  and
               Wei, Furu  and
               Deng, Weiwei  and
               Zhang, Qi},
  editor    = {Bouamor, Houda  and
               Pino, Juan  and
               Bali, Kalika},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.emnlp-main.758},
  doi       = {10.18653/v1/2023.emnlp-main.758},
  pages     = {12318--12337}
}

@article{cheng2024lift,
  title   = {Lift yourself up: Retrieval-augmented text generation with self-memory},
  author  = {Cheng, Xin and Luo, Di and Chen, Xiuying and Liu, Lemao and Zhao, Dongyan and Yan, Rui},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2024},
  url     = {https://openreview.net/forum?id=lYNSvp51a7}
}

@inproceedings{lin2024radit,
  title     = {{RA}-{DIT}: Retrieval-Augmented Dual Instruction Tuning},
  author    = {Xi Victoria Lin and Xilun Chen and Mingda Chen and Weijia Shi and Maria Lomeli and Richard James and Pedro Rodriguez and Jacob Kahn and Gergely Szilvasy and Mike Lewis and Luke Zettlemoyer and Wen-tau Yih},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year      = {2024},
  url       = {https://openreview.net/forum?id=22OTbutug9}
}


@article{zhang2023miracl,
  author  = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},
  title   = {{MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages}},
  journal = {Transactions of the Association for Computational Linguistics},
  volume  = {11},
  pages   = {1114-1131},
  year    = {2023},
  month   = {09},
  issn    = {2307-387X},
  doi     = {10.1162/tacl_a_00595},
  url     = {https://doi.org/10.1162/tacl\_a\_00595},
  eprint  = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00595/2157340/tacl\_a\_00595.pdf}
}


@misc{bonifacio2021mmarco,
  title         = {mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset},
  author        = {Luiz Henrique Bonifacio and Vitor Jeronymo and Hugo Queiroz Abonizio and Israel Campiotti and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira},
  year          = {2021},
  eprint        = {2108.13897},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{yang-etal-2023-multi-eup,
  title     = {Multi-{E}u{P}: The Multilingual {E}uropean Parliament Dataset for Analysis of Bias in Information Retrieval},
  author    = {Yang, Jinrui  and
               Baldwin, Timothy  and
               Cohn, Trevor},
  editor    = {Ataman, Duygu},
  booktitle = {Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL)},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.mrl-1.21},
  doi       = {10.18653/v1/2023.mrl-1.21},
  pages     = {282--291}
}


@inproceedings{kamateri2019test,
  author    = {Kamateri, Eleni and Tsikrika, Theodora and Symeonidis, Spyridon and Vrochidis, Stefanos and Minker, Wolfgang and Kompatsiaris, Yiannis},
  title     = {A Test Collection for Passage Retrieval Evaluation of Spanish Health-Related Resources},
  year      = {2019},
  isbn      = {978-3-030-15718-0},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  url       = {https://doi.org/10.1007/978-3-030-15719-7_19},
  doi       = {10.1007/978-3-030-15719-7_19},
  abstract  = {This paper describes a new test collection for passage retrieval from health-related Web resources in Spanish. The test collection contains 10,037 health-related documents in Spanish, 37 topics representing complex information needs formulated in a total of 167 natural language questions, and manual relevance assessments of text passages, pooled from multiple systems. This test collection is the first to combine search in a language beyond English, passage retrieval, and health-related resources and topics targeting the general public.},
  booktitle = {Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019, Cologne, Germany, April 14–18, 2019, Proceedings, Part II},
  pages     = {148–154},
  numpages  = {7},
  keywords  = {Test collection, Passage retrieval, Inter-rater agreement},
  location  = {Cologne, Germany}
}

@article{gutierrez2022maria,
  author   = {Guti{\'e}rrez-Fandi{\~n}o, Asier and Armengol-Estap{\'e}, Jordi and P{\`a}mies, Marc and Llop-Palao, Joan and Silveira-Ocampo, Joaquin and Carrino, Casimiro Pio and Gonzalez-Agirre, Aitor and Armentano-Oller, Carme and Rodriguez-Penagos, Carlos and Villegas, Marta},
  title    = {MarIA: Spanish Language Models},
  journal  = {Procesamiento del Lenguaje Natural},
  volume   = {68},
  number   = {0},
  year     = {2022},
  keywords = {},
  abstract = {This work presents MarIA, a family of Spanish language models and associated resources made available to the industry and the research community. Currently, MarIA includes RoBERTa-base, RoBERTa-large, GPT2 and GPT2-large Spanish language models, which can arguably be presented as the largest and most proficient language models in Spanish. The models were pretrained using a massive corpus of 570GB of clean and deduplicated texts with 135 billion words extracted from the Spanish Web Archive crawled by the National Library of Spain between 2009 and 2019. We assessed the performance of the models with nine existing evaluation datasets and with a novel extractive Question Answering dataset created ex novo. Overall, MarIA models outperform the existing Spanish models across a variety of NLU tasks and training settings.},
  issn     = {1989-7553},
  url      = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6405},
  pages    = {39--60}
}

@article{bajaj2016ms,
  title   = {Ms marco: A human generated machine reading comprehension dataset},
  author  = {Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
  journal = {arXiv preprint arXiv:1611.09268},
  year    = {2016},
  url     = {https://arxiv.org/abs/1611.09268}
}

@misc{attardi2015wikiextractor,
  author       = {Giusepppe Attardi},
  title        = {WikiExtractor},
  year         = {2015},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/attardi/wikiextractor}}
}

@inproceedings{rybak-2023-maupqa,
  title     = {{MAUPQA}: Massive Automatically-created {P}olish Question Answering Dataset},
  author    = {Rybak, Piotr},
  editor    = {Piskorski, Jakub  and
               Marci{\'n}czuk, Micha{\l}  and
               Nakov, Preslav  and
               Ogrodniczuk, Maciej  and
               Pollak, Senja  and
               P{\v{r}}ib{\'a}{\v{n}}, Pavel  and
               Rybak, Piotr  and
               Steinberger, Josef  and
               Yangarber, Roman},
  booktitle = {Proceedings of the 9th Workshop on Slavic Natural Language Processing 2023 (SlavicNLP 2023)},
  month     = may,
  year      = {2023},
  address   = {Dubrovnik, Croatia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.bsnlp-1.2},
  doi       = {10.18653/v1/2023.bsnlp-1.2},
  pages     = {11--16},
  abstract  = {Recently, open-domain question answering systems have begun to rely heavily on annotated datasets to train neural passage retrievers. However, manually annotating such datasets is both difficult and time-consuming, which limits their availability for less popular languages. In this work, we experiment with several methods for automatically collecting weakly labeled datasets and show how they affect the performance of the neural passage retrieval models. As a result of our work, we publish the MAUPQA dataset, consisting of nearly 400,000 question-passage pairs for Polish, as well as the HerBERT-QA neural retriever.}
}

