{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all datasets in appropriate format:\n",
    "\n",
    "* Folder data/dataset_corpus/ with docs-**.jsonl.gz for each dataset with fields docid, title, text. (FILL TITLE WITH EMPTY IF MISSING)\n",
    "* Folder data/dataset_eval/ with \n",
    "    * queries.dataset.csv with cols id, query, WITH header.  COMMA SEP.\n",
    "    * topics.dataset.tsv con qid, query, NO header. TAB SEP.\n",
    "    * qrels.dataset.tsv con qid, Q0, docid, rel (only 1s), NO header.  TAB SEP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500_000\n",
    "\n",
    "def write_batch_to_jsonl_gz(docs, outdir, file_index):\n",
    "    out_file = Path(outdir) / f\"docs-{file_index:02d}.jsonl.gz\"\n",
    "    with gzip.open(out_file, \"wt\", encoding=\"utf-8\") as f_out:\n",
    "        for doc in docs:\n",
    "            f_out.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mmarco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_outdir = \"../../data/mmarco_corpus\"\n",
    "eval_outdir = \"../../data/mmarco_eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# DOCS (fields: text)\n",
    "mmarco_collection = datasets.load_dataset('unicamp-dl/mmarco', 'collection-spanish', trust_remote_code=True)\n",
    "ds_docs = mmarco_collection[\"collection\"]\n",
    "ds_docs = ds_docs.rename_column(\"id\", \"docid\")\n",
    "ds_docs = ds_docs.add_column(\"title\", [\"\"] * len(ds_docs))\n",
    "\n",
    "n_docs = len(ds_docs)\n",
    "\n",
    "end_start_indices = [(i, i + batch_size) for i in range(0, n_docs, batch_size)]\n",
    "\n",
    "Path(docs_outdir).mkdir(exist_ok=True, parents=True)\n",
    "for file_index, (start, end) in enumerate(end_start_indices):\n",
    "    end_ = min(end, n_docs)\n",
    "    docs = ds_docs.select(range(start, end_))\n",
    "    write_batch_to_jsonl_gz(docs, docs_outdir, file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# QUERIES\n",
    "ds_queries = datasets.load_dataset('unicamp-dl/mmarco', 'queries-spanish', trust_remote_code=True)\n",
    "df_queries = ds_queries[\"dev\"].to_pandas()\n",
    "df_queries = df_queries.rename(columns={\"text\": \"query\"})\n",
    "\n",
    "Path(eval_outdir).mkdir(exist_ok=True, parents=True)\n",
    "df_queries.to_csv(f\"{eval_outdir}/queries.mmarco.csv\", header=True, index=False)\n",
    "df_queries.to_csv(f\"{eval_outdir}/topics.mmarco.tsv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QRELS\n",
    "df_qrels = pd.read_csv(\n",
    "    \"https://huggingface.co/datasets/unicamp-dl/mmarco/resolve/main/data/qrels.dev.small.tsv\",\n",
    "    # \"https://huggingface.co/datasets/unicamp-dl/mmarco/resolve/main/data/qrels.dev.tsv\",\n",
    "    sep=\"\\t\", header=None, names=[\"id\", \"q0\", \"docid\", \"rel\"],\n",
    ")\n",
    "df_qrels = df_qrels.query(\"rel > 0\")\n",
    "\n",
    "df_qrels.to_csv(f\"{eval_outdir}/qrels.mmarco.tsv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_outdir = \"../../data/pres_corpus\"\n",
    "eval_outdir = \"../../data/pres_eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# DOCS (fields: text)\n",
    "pres_corpus = datasets.load_dataset(\"jinaai/spanish_passage_retrieval\", \"corpus.documents\", trust_remote_code=True)\n",
    "ds_docs = pres_corpus[\"test\"]\n",
    "ds_docs = ds_docs.rename_column(\"_id\", \"docid\")\n",
    "ds_docs = ds_docs.add_column(\"title\", [\"\"] * len(ds_docs))\n",
    "\n",
    "n_docs = len(ds_docs)\n",
    "\n",
    "end_start_indices = [(i, i + batch_size) for i in range(0, n_docs, batch_size)]\n",
    "\n",
    "Path(docs_outdir).mkdir(exist_ok=True, parents=True)\n",
    "for file_index, (start, end) in enumerate(end_start_indices):\n",
    "    end_ = min(end, n_docs)\n",
    "    docs = ds_docs.select(range(start, end_))\n",
    "    write_batch_to_jsonl_gz(docs, docs_outdir, file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_docs.to_pandas()[\"docid\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# QUERIES\n",
    "pres_queries = datasets.load_dataset(\"jinaai/spanish_passage_retrieval\", \"queries\", trust_remote_code=True)\n",
    "df_queries = pres_queries[\"test\"].to_pandas().rename(\n",
    "    columns={\"_id\": \"id\", \"text\": \"query\"}\n",
    ")\n",
    "\n",
    "Path(eval_outdir).mkdir(exist_ok=True, parents=True)\n",
    "df_queries.to_csv(f\"{eval_outdir}/queries.pres.csv\", header=True, index=False)\n",
    "df_queries.to_csv(f\"{eval_outdir}/topics.pres.tsv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_queries[\"id\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Qrels\n",
    "pres_qrels = datasets.load_dataset(\"jinaai/spanish_passage_retrieval\", \"qrels.s2p\", trust_remote_code=True)\n",
    "df_qrels = pres_qrels[\"test\"].to_pandas()\n",
    "df_qrels[\"text\"] = df_qrels[\"text\"].str.split(\" \")\n",
    "df_qrels = df_qrels.explode(\"text\").reset_index(drop=True).rename(\n",
    "    columns={\"_id\": \"id\", \"text\": \"docid\"}\n",
    ")\n",
    "df_qrels[\"q0\"] = \"Q0\"\n",
    "df_qrels[\"rel\"] = 1\n",
    "df_qrels = df_qrels[[\"id\", \"q0\", \"docid\", \"rel\"]]\n",
    "df_qrels = df_qrels.query(\"rel > 0\")\n",
    "\n",
    "df_qrels = df_qrels.drop_duplicates([\"id\", \"docid\"])\n",
    "\n",
    "df_qrels.to_csv(f\"{eval_outdir}/qrels.pres.tsv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_qrels[[\"id\", \"docid\"]].duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIRACL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_outdir = \"../../data/miracl_corpus\"\n",
    "eval_outdir = \"../../data/miracl_eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DOCS (fields: title, text)\n",
    "ds_docs = datasets.load_dataset(\"miracl/miracl-corpus\", \"es\", trust_remote_code=True)[\"train\"]\n",
    "\n",
    "n_docs = len(ds_docs)\n",
    "\n",
    "end_start_indices = [(i, i + batch_size) for i in range(0, n_docs, batch_size)]\n",
    "\n",
    "Path(docs_outdir).mkdir(exist_ok=True, parents=True)\n",
    "for file_index, (start, end) in enumerate(end_start_indices):\n",
    "    end_ = min(end, n_docs)\n",
    "    docs = ds_docs.select(range(start, end_))\n",
    "    write_batch_to_jsonl_gz(docs, docs_outdir, file_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUERIES\n",
    "df_queries = pd.read_csv(\n",
    "    f\"../../data/miracl/miracl-v1.0-es/topics/topics.miracl-v1.0-es-dev.tsv\",\n",
    "    sep=\"\\t\", header=None, names=[\"id\", \"query\"],\n",
    ")\n",
    "\n",
    "Path(eval_outdir).mkdir(exist_ok=True, parents=True)\n",
    "df_queries.to_csv(f\"{eval_outdir}/queries.miracl.csv\", header=True, index=False)\n",
    "df_queries.to_csv(f\"{eval_outdir}/topics.miracl.tsv\", sep=\"\\t\", header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QRELS\n",
    "df_qrels = pd.read_csv(\n",
    "    f\"../../data/miracl/miracl-v1.0-es/qrels/qrels.miracl-v1.0-es-dev.tsv\",\n",
    "    sep=\"\\t\", header=None, names=[\"id\", \"q0\", \"docid\", \"rel\"],\n",
    ")\n",
    "df_qrels = df_qrels.query(\"rel > 0\")\n",
    "\n",
    "df_qrels.to_csv(f\"{eval_outdir}/qrels.miracl.tsv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-EuParl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_outdir = \"../../data/meup_corpus\"\n",
    "eval_outdir = \"../../data/meup_eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "meup = datasets.load_dataset(\"unimelb-nlp/Multi-EuP\", keep_default_na=False)\n",
    "meup = meup[\"full\"].select_columns([\"TEXT\", \"did\", \"title_ES\", \"qid_ES\", \"LANGUAGE\"])\n",
    "df_meup = meup.to_pandas().query(\"LANGUAGE == 'ES'\").rename(\n",
    "    columns={\"TEXT\": \"text\", \"did\": \"docid\", \"title_ES\": \"query\", \"qid_ES\": \"id\"}\n",
    ").drop(columns=\"LANGUAGE\")\n",
    "\n",
    "### DOCS\n",
    "df_docs = df_meup[[\"docid\", \"text\"]].drop_duplicates()\n",
    "df_docs[\"title\"] = \"\"\n",
    "ds_docs = datasets.Dataset.from_pandas(df_docs)\n",
    "n_docs = len(ds_docs)\n",
    "end_start_indices = [(i, i + batch_size) for i in range(0, n_docs, batch_size)]\n",
    "Path(docs_outdir).mkdir(exist_ok=True, parents=True)\n",
    "for file_index, (start, end) in enumerate(end_start_indices):\n",
    "    end_ = min(end, n_docs)\n",
    "    docs = ds_docs.select(range(start, end_))\n",
    "    write_batch_to_jsonl_gz(docs, docs_outdir, file_index)\n",
    "\n",
    "### QUERIES\n",
    "df_queries = df_meup[[\"id\", \"query\"]].drop_duplicates()\n",
    "Path(eval_outdir).mkdir(exist_ok=True, parents=True)\n",
    "df_queries.to_csv(f\"{eval_outdir}/queries.meup.csv\", header=True, index=False)\n",
    "df_queries.to_csv(f\"{eval_outdir}/topics.meup.tsv\", sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "### QRELS\n",
    "df_qrels = df_meup[[\"id\", \"docid\"]].drop_duplicates()\n",
    "df_qrels[\"q0\"] = \"Q0\"\n",
    "df_qrels[\"rel\"] = 1\n",
    "df_qrels = df_qrels[[\"id\", \"q0\", \"docid\", \"rel\"]]\n",
    "df_qrels = df_qrels.query(\"rel > 0\")\n",
    "\n",
    "df_qrels.to_csv(f\"{eval_outdir}/qrels.meup.tsv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_outdir = \"../../data/sqac_corpus\"\n",
    "eval_outdir = \"../../data/sqac_eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqac_examples(sqac_data):\n",
    "    \"\"\"This function returns the examples in the raw (text) form.\"\"\"\n",
    "    for article in sqac_data:\n",
    "        title = article.get(\"title\", \"\").strip()\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"].strip()\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question = qa[\"question\"].strip()\n",
    "                id_ = qa[\"id\"]\n",
    "\n",
    "                answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
    "                answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]\n",
    "\n",
    "                # Features currently used are \"context\", \"question\", and \"answers\".\n",
    "                # Others are extracted here for the ease of future expansions.\n",
    "                yield id_, {\n",
    "                    \"title\": title,\n",
    "                    \"context\": context,\n",
    "                    \"question\": question,\n",
    "                    \"id\": id_,\n",
    "                    \"answers\": {\n",
    "                        \"answer_start\": answer_starts,\n",
    "                        \"text\": answers,\n",
    "                    },\n",
    "                }\n",
    "\n",
    "def get_sqac(split):\n",
    "    sqac = pd.read_json(f\"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/{split}.json\")\n",
    "    sqac_data = sqac[\"data\"].tolist()\n",
    "    examples = list(sqac_examples(sqac_data))\n",
    "    formatted_data = [\n",
    "        {\n",
    "            'id': item[0],\n",
    "            'title': item[1]['title'],\n",
    "            'text': item[1]['context'],\n",
    "            'query': item[1]['question'],\n",
    "        }\n",
    "        for item in examples\n",
    "    ]\n",
    "    df_sqac = pd.json_normalize(formatted_data)\n",
    "    return df_sqac\n",
    "\n",
    "df_sqac = pd.DataFrame()\n",
    "for split in [\"train\", \"dev\", \"test\"]:\n",
    "    df_tmp = get_sqac(split)\n",
    "    df_tmp[\"split\"] = split\n",
    "    df_sqac = pd.concat([df_sqac, df_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>query</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6cf3dcd6-b5a3-4516-8f9e-c5c1c6b66628</td>\n",
       "      <td>Historia de Japón</td>\n",
       "      <td>La historia de Japón (日本の歴史 o 日本史, Nihon no re...</td>\n",
       "      <td>¿Qué influencia convirtió Japón en una nación ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2663226e-e652-43a2-a6ba-c1fd02a1df31</td>\n",
       "      <td>Historia de Japón</td>\n",
       "      <td>La historia de Japón (日本の歴史 o 日本史, Nihon no re...</td>\n",
       "      <td>¿Cuándo se detuvo el expansionismo de Japón?</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id              title  \\\n",
       "0  6cf3dcd6-b5a3-4516-8f9e-c5c1c6b66628  Historia de Japón   \n",
       "1  2663226e-e652-43a2-a6ba-c1fd02a1df31  Historia de Japón   \n",
       "\n",
       "                                                text  \\\n",
       "0  La historia de Japón (日本の歴史 o 日本史, Nihon no re...   \n",
       "1  La historia de Japón (日本の歴史 o 日本史, Nihon no re...   \n",
       "\n",
       "                                               query  split  \n",
       "0  ¿Qué influencia convirtió Japón en una nación ...  train  \n",
       "1       ¿Cuándo se detuvo el expansionismo de Japón?  train  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sqac.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE I will make my own ids because it's not clear what ID col stands for.\n",
    "df_sqac['id'] = pd.factorize(df_sqac['query'])[0]\n",
    "df_sqac['docid'] = pd.factorize(df_sqac['title'] + \" \" + df_sqac['text'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DOCS\n",
    "df_docs = df_sqac[[\"docid\", \"title\", \"text\"]].drop_duplicates()\n",
    "ds_docs = datasets.Dataset.from_pandas(df_docs)\n",
    "n_docs = len(ds_docs)\n",
    "end_start_indices = [(i, i + batch_size) for i in range(0, n_docs, batch_size)]\n",
    "Path(docs_outdir).mkdir(exist_ok=True, parents=True)\n",
    "for file_index, (start, end) in enumerate(end_start_indices):\n",
    "    end_ = min(end, n_docs)\n",
    "    docs = ds_docs.select(range(start, end_))\n",
    "    write_batch_to_jsonl_gz(docs, docs_outdir, file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUERIES\n",
    "df_queries = df_sqac.query(\"split == 'test'\")[[\"id\", \"query\"]].drop_duplicates()\n",
    "Path(eval_outdir).mkdir(exist_ok=True, parents=True)\n",
    "df_queries.to_csv(f\"{eval_outdir}/queries.sqac.csv\", header=True, index=False)\n",
    "df_queries.to_csv(f\"{eval_outdir}/topics.sqac.tsv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QRELS\n",
    "df_qrels = df_sqac.query(\"split == 'test'\")[[\"id\", \"docid\"]].drop_duplicates()\n",
    "df_qrels[\"q0\"] = \"Q0\"\n",
    "df_qrels[\"rel\"] = 1\n",
    "df_qrels = df_qrels[[\"id\", \"q0\", \"docid\", \"rel\"]]\n",
    "df_qrels = df_qrels.query(\"rel > 0\")\n",
    "\n",
    "df_qrels.to_csv(f\"{eval_outdir}/qrels.sqac.tsv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 14934 queries, 15036 docs, J/D: 1.01\n",
      "dev: 1861 queries, 1864 docs, J/D: 1.00\n",
      "test: 1908 queries, 1910 docs, J/D: 1.00\n"
     ]
    }
   ],
   "source": [
    "# for split in [\"train\", \"dev\", \"test\"]:\n",
    "#     n_queries = df_sqac[df_sqac[\"split\"] == split][\"query\"].nunique()\n",
    "#     n_docs = len(df_sqac[df_sqac[\"split\"] == split][\"text\"])\n",
    "#     print(f\"{split}: {n_queries} queries, {n_docs} docs, J/D: {n_docs / n_queries:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spanish-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
